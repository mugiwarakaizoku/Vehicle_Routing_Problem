# -*- coding: utf-8 -*-
"""model_building.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sELINJyEjpAmNCIez2HL7r3vcAQAHv_c
"""

TOTAL_STOPS = rewards.shape[0]
DEPO = 0
INPUT_VECTOR_SHAPE = rewards.shape[0]

# REPLAY_MEMORY_SIZE = 300
# MODEL_NAME = 'NN_'
# MIN_REPLAY_MEMORY_SIZE = 300
# MINI_BATCH_SIZE = 50
LEARNING_RATE_OF_NN = 0.0000001
# UPDATE_TARGET_WEIGHTS_AFTER_EVERY_STEPS = 10
TOTAL_EPISODES = 30
AGGREGATE_STATS_EVERY = 10

GAMMA = 0.99
EPSILON = 1
EPSILON_DECAY = 0.975
MIN_EPSILON = 0.001

LOAD_CHECK = False

distance = np.array([[0, 2451, 713, 1018, 1631, 1374, 2408, 213, 2571, 875, 1420, 2145, 1972],
        [2451, 0, 1745, 1524, 831, 1240, 959, 2596, 403, 1589, 1374, 357, 579],
        [713, 1745, 0, 355, 920, 803, 1737, 851, 1858, 262, 940, 1453, 1260],
        [1018, 1524, 355, 0, 700, 862, 1395, 1123, 1584, 466, 1056, 1280, 987],
        [1631, 831, 920, 700, 0, 663, 1021, 1769, 949, 796, 879, 586, 371],
        [1374, 1240, 803, 862, 663, 0, 1681, 1551, 1765, 547, 225, 887, 999],
        [2408, 959, 1737, 1395, 1021, 1681, 0, 2493, 678, 1724, 1891, 1114, 701],
        [213, 2596, 851, 1123, 1769, 1551, 2493, 0, 2699, 1038, 1605, 2300, 2099],
        [2571, 403, 1858, 1584, 949, 1765, 678, 2699, 0, 1744, 1645, 653, 600],
        [875, 1589, 262, 466, 796, 547, 1724, 1038, 1744, 0, 679, 1272, 1162],
        [1420, 1374, 940, 1056, 879, 225, 1891, 1605, 1645, 679, 0, 1017, 1200],
        [2145, 357, 1453, 1280, 586, 887, 1114, 2300, 653, 1272, 1017, 0, 504],
        [1972, 579, 1260, 987, 371, 999, 701, 2099, 600, 1162, 1200, 504, 0]])

import tensorflow as tf

print(tf.test.gpu_device_name())
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm() 

import os
import tensorflow.keras as keras
from tensorflow.keras.layers import Dense
import tensorflow_probability as tfp
import numpy as np
import random
from tensorflow.keras.optimizers import Adam
from agent.py import Agent
from actor_critic_network.py import ActorCriticNetwork
from tqdm import tqdm

rewards = np.zeros(distance.shape)
for i in range(rewards.shape[0]):
    for j in range(rewards[0].shape[0]):
        rewards[i][j]=-distance[i][j]
rewards

def convert_station_to_vector(station):
    vector_rep = np.zeros(TOTAL_STOPS)
    vector_rep[station]=1
    return vector_rep

agent = Agent()
ep_rewards = []

if LOAD_CHECK:
  agent.load_model()
for episode in tqdm(range(1,TOTAL_EPISODES+1),ascii=True,unit='episodes'):
    visited_states_set = set()
    episode_reward = 0
    current_stop = DEPO
    current_state = convert_station_to_vector(DEPO)
#     print('here_1')
    
    done = False
    while not done:
        action = agent.chose_an_action(current_state)
        new_state = action
        new_state = convert_station_to_vector(action)
        visited_states_set.add(action)
        if len(visited_states_set) == 13:
            done=True
#         print(visited_states_set,done)
        reward = rewards[current_stop][action]
        episode_reward+=reward

        if not LOAD_CHECK:
          agent.learning(current_state,reward,new_state,done)


        current_state = new_state
        current_stop = action

    if not LOAD_CHECK:
      agent.save_model()
      
    
    ep_rewards.append(episode_reward)
    if not episode % AGGREGATE_STATS_EVERY or episode == 1:
        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])
        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])
        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])
    
    if EPSILON > MIN_EPSILON:
        EPSILON *= EPSILON_DECAY
        EPSILON = max(MIN_EPSILON, EPSILON)

#getting the route
path=[0]
all_states=[i for i in range(0,13)]
while len(path)!=13:
  remaining_states=np.delete(all_states,path)
  action_index_in_remaining_states = np.argmax(np.delete(agent.make_prediction(convert_station_to_vector(path[-1]))[1], path))
  action = remaining_states[action_index_in_remaining_states]
  path.append(action)
print(path)

#getting the total distance covered
distance_travelled=0
for i in range(len(path)-1):
    distance_travelled+=distance[path[i]][path[i+1]]
print(travedistance_travelled)

print(agent.actor_critic.trainable_variables)