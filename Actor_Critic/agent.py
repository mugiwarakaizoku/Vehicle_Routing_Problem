# -*- coding: utf-8 -*-
"""agent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11lZwv89KhS-ukseGWRp4BTVdudhfi_AH
"""

from actor_critic_network.py import ActorCriticNetwork

class Agent:
  def __init__(self,lr = LEARNING_RATE_OF_NN,gamma= GAMMA,n_actions=TOTAL_STOPS,epsilon=EPSILON,eps_decay = EPSILON_DECAY,min_eps = MIN_EPSILON):
     self.gamma = gamma
     self.lr = lr
     self.n_actions = n_actions
     self.action = None
     self.epsilon = epsilon
     self.eps_decay = eps_decay
     self.min_eps = min_eps
     self.action_space = [i for i in range(n_actions)]
     self.actor_critic = ActorCriticNetwork(n_actions=n_actions)
     self.actor_critic.compile(optimizer=Adam(learning_rate=lr))
  
  def chose_an_action(self,env_observation):
    state = tf.convert_to_tensor([env_observation])
    _,prob_of_actions = self.actor_critic(state)
    action_probabilities = tfp.distributions.Categorical(probs = prob_of_actions)
    action = random.randint(0,len(prob_of_actions[0])-1)
    self.action = action
    return action

  def save_model(self):
     self.actor_critic.save_weights(self.actor_critic.check_point_file)

  def load_model(self):
    print('...loading model....')
    self.actor_critic.load_weights(self.actor_critic.check_point_file)
  
  def make_prediction(self,state):
    state = tf.convert_to_tensor([state],dtype=tf.float32)
    prediction = self.actor_critic.predict(state)
    self.preditction = prediction
    return prediction

  def learning(self,state,reward,next_state,done):
    state = tf.convert_to_tensor([state],dtype = tf.float32)
    next_state = tf.convert_to_tensor([next_state],dtype = tf.float32)
    reward = tf.convert_to_tensor(reward,dtype = tf.float32)
    
    with tf.GradientTape() as tape:
      value_fn_of_state,probs = self.actor_critic.call(state)
      value_fn_of_next_state,_ = self.actor_critic.call(next_state)
      value_fn_of_state = tf.squeeze(value_fn_of_state)
      value_fn_of_next_state = tf.squeeze(value_fn_of_next_state)

      action_probs = tfp.distributions.Categorical(probs = probs)
      log_prob = action_probs.log_prob(self.action)

      delta = reward + self.gamma*value_fn_of_next_state*(1-int(done))-value_fn_of_state
      actor_loss = -log_prob*(delta)
      critic_loss = -delta**2
      total_loss = actor_loss+critic_loss
      
      gradient = tape.gradient(total_loss,self.actor_critic.trainable_variables)
      self.actor_critic.optimizer.apply_gradients(zip(
          gradient,self.actor_critic.trainable_variables
      ))