# -*- coding: utf-8 -*-
"""dqn_agent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HRY4TquXgO37mtEiKkYVvNCHcAo4bhMu
"""

import tensorflow as tf
from keras.layers import Dense 
from tensorflow import keras
from tensorflow.keras.models import Sequential
import numpy as np
import time
from modified_tensorboard import ModifiedTensorBoard
from collections import deque
import random


class DQN_agent:
    def __init__(self):
        self.model = self.create_model()

        self.target_model = self.create_model()
        self.target_model.set_weights(self.model.get_weights())

        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)

        self.tensorboard = ModifiedTensorBoard(log_dir=f"logs/{MODEL_NAME}--{int(time.time())}")

        self.target_update_counter = 0

    def create_model(self):
      with tf.device('/gpu:0'):
        model =  Sequential()
        model.add(Dense(10,input_shape=(INPUT_VECTOR_SHAPE,),activation='relu'))
        model.add(Dense(TOTAL_STOPS,activation=None))
        model.compile(loss='mse',optimizer=keras.optimizers.Adam(LEARNING_RATE_OF_NN),metrics=['mae'])
        return model

    def update_replay_memory(self,transition):
        self.replay_memory.append(transition)

    def get_qs(self,state):
        return self.model.predict(np.array(state).reshape(-1,*state.shape))[0]

    def train(self,terminal_state,step):
        if len(self.replay_memory)<MIN_REPLAY_MEMORY_SIZE:
            return
        minibatch = random.sample(self.replay_memory,MINI_BATCH_SIZE)

        current_states = np.array([transition[0] for transition in minibatch])
        current_qs_list = self.model.predict(current_states)

        new_current_states = np.array([transition[3] for transition in minibatch]) #transition = curr_state,action,reward,next_state
        future_qs_list = self.target_model.predict(new_current_states)

        x=[]
        y=[]

        for index, (current_states,action,reward,new_current_states,done) in enumerate(minibatch):
            if not done:
                max_future_q = np.max(future_qs_list[index])
                new_q = reward + DISCOUNT_FACTOR*max_future_q
            else:
                new_q = reward
            current_qs = current_qs_list[index]
            current_qs_action = new_q

            x.append(current_states)
            y .append(current_qs)
        self.model.fit(np.array(x),np.array(y),batch_size = MINI_BATCH_SIZE,
                   verbose=0,shuffle=False,callbacks=[self.tensorboard] if terminal_state else None)
        if terminal_state:
            self.target_update_counter+=1
    
        if self.target_update_counter > UPDATE_TARGET_WEIGHTS_AFTER_EVERY_STEPS:
            self.target_model.set_weights(self.model.get_weights ())
            self.target_update_counter = 0