# -*- coding: utf-8 -*-
"""Actor_critic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WlLzvfNRM8u6tu3dnyA5lSgAnU6P88fi
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf
print(tf.version)

# try:
#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
# except ValueError:
#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)

print(tf.test.gpu_device_name())
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

import os
import tensorflow.keras as keras
from tensorflow.keras.layers import Dense

import tensorflow_probability as tfp

import numpy as np

import random
from tensorflow.keras.optimizers import Adam

distance = np.array([[0, 2451, 713, 1018, 1631, 1374, 2408, 213, 2571, 875, 1420, 2145, 1972],
        [2451, 0, 1745, 1524, 831, 1240, 959, 2596, 403, 1589, 1374, 357, 579],
        [713, 1745, 0, 355, 920, 803, 1737, 851, 1858, 262, 940, 1453, 1260],
        [1018, 1524, 355, 0, 700, 862, 1395, 1123, 1584, 466, 1056, 1280, 987],
        [1631, 831, 920, 700, 0, 663, 1021, 1769, 949, 796, 879, 586, 371],
        [1374, 1240, 803, 862, 663, 0, 1681, 1551, 1765, 547, 225, 887, 999],
        [2408, 959, 1737, 1395, 1021, 1681, 0, 2493, 678, 1724, 1891, 1114, 701],
        [213, 2596, 851, 1123, 1769, 1551, 2493, 0, 2699, 1038, 1605, 2300, 2099],
        [2571, 403, 1858, 1584, 949, 1765, 678, 2699, 0, 1744, 1645, 653, 600],
        [875, 1589, 262, 466, 796, 547, 1724, 1038, 1744, 0, 679, 1272, 1162],
        [1420, 1374, 940, 1056, 879, 225, 1891, 1605, 1645, 679, 0, 1017, 1200],
        [2145, 357, 1453, 1280, 586, 887, 1114, 2300, 653, 1272, 1017, 0, 504],
        [1972, 579, 1260, 987, 371, 999, 701, 2099, 600, 1162, 1200, 504, 0]])
print(distance.shape)

rewards = np.zeros(distance.shape)
for i in range(rewards.shape[0]):
    for j in range(rewards[0].shape[0]):
        rewards[i][j]=-distance[i][j]
# rewards = rewards/1000
rewards

TOTAL_STOPS = rewards.shape[0]
DEPO = 0
INPUT_VECTOR_SHAPE = rewards.shape[0]

# REPLAY_MEMORY_SIZE = 300
# MODEL_NAME = 'NN_'
# MIN_REPLAY_MEMORY_SIZE = 300
# MINI_BATCH_SIZE = 50
LEARNING_RATE_OF_NN = 0.0000001
# UPDATE_TARGET_WEIGHTS_AFTER_EVERY_STEPS = 10
TOTAL_EPISODES = 30
AGGREGATE_STATS_EVERY = 10

GAMMA = 0.99
EPSILON = 1
EPSILON_DECAY = 0.975
MIN_EPSILON = 0.001

LOAD_CHECK = False

#fc_1 => fully connected layer 1
#v=> value_function, pi is policy
class ActorCriticNetwork(keras.Model):
  def __init__(self,n_actions,layer_1_dms=10,layer_2_dms=5,name='actor_critic',
               checkpoint_dir='/content/tmp/actor_critic'):
    super(ActorCriticNetwork,self).__init__()
    self.layer_1_dms = layer_1_dms
    self.layer_2_dms = layer_2_dms
    self.n_actions = n_actions
    self.model_name = name
    self.check_point_dir = checkpoint_dir
    self.check_point_file = os.path.join(self.check_point_dir+name+'_ac')
    self.fc1 = Dense(self.layer_1_dms,activation='relu')
    self.fc2 = Dense(self.layer_2_dms,activation='relu')
    self.v=Dense(1,activation=None)
    self.pi = Dense(self.n_actions,activation='softmax')
  def call(self,state):
     value = self.fc1(state)
     value = self.fc2(value)

     value_fn = self.v(value)
     policy_pi = self.pi(value)
     value_fn = value_fn*100000
     policy_pi = policy_pi*100000
     print(policy_pi,value_fn)

     return value_fn,policy_pi

def convert_station_to_vector(station):
    vector_rep = np.zeros(TOTAL_STOPS)
    vector_rep[station]=1
    return vector_rep

class Agent:
  def __init__(self,lr = LEARNING_RATE_OF_NN,gamma= GAMMA,n_actions=TOTAL_STOPS,epsilon=EPSILON,eps_decay = EPSILON_DECAY,min_eps = MIN_EPSILON):
     self.gamma = gamma
     self.lr = lr
     self.n_actions = n_actions
     self.action = None
     self.epsilon = epsilon
     self.eps_decay = eps_decay
     self.min_eps = min_eps
     self.action_space = [i for i in range(n_actions)]
     self.actor_critic = ActorCriticNetwork(n_actions=n_actions)
     self.actor_critic.compile(optimizer=Adam(learning_rate=lr))
  
  def chose_an_action(self,env_observation):
    state = tf.convert_to_tensor([env_observation])
    _,prob_of_actions = self.actor_critic(state)
    action_probabilities = tfp.distributions.Categorical(probs = prob_of_actions)
    # print(prob_of_actions,len(prob_of_actions[0]))
    # action = action_probabilities.sample()
    # action = random.choice(prob_of_actions[0])
    action = random.randint(0,len(prob_of_actions[0])-1)
    # print(action)
    self.action = action
    # action = action.numpy()[0]
    return action

  def save_model(self):
    #  print('..saving models ...')
     self.actor_critic.save_weights(self.actor_critic.check_point_file)

  def load_model(self):
    print('...loading model....')
    self.actor_critic.load_weights(self.actor_critic.check_point_file)
  
  def make_prediction(self,state):
    state = tf.convert_to_tensor([state],dtype=tf.float32)
    prediction = self.actor_critic.predict(state)
    self.preditction = prediction
    return prediction

  def learning(self,state,reward,next_state,done):
    # state = convert_station_to_vector(state)
    # next_state = convert_station_to_vector(next_state)
    state = tf.convert_to_tensor([state],dtype = tf.float32)
    next_state = tf.convert_to_tensor([next_state],dtype = tf.float32)
    reward = tf.convert_to_tensor(reward,dtype = tf.float32)
    
    with tf.GradientTape() as tape:
      value_fn_of_state,probs = self.actor_critic.call(state)
      value_fn_of_next_state,_ = self.actor_critic.call(next_state)
      value_fn_of_state = tf.squeeze(value_fn_of_state)
      value_fn_of_next_state = tf.squeeze(value_fn_of_next_state)

      action_probs = tfp.distributions.Categorical(probs = probs)
      log_prob = action_probs.log_prob(self.action)

      delta = reward + self.gamma*value_fn_of_next_state*(1-int(done))-value_fn_of_state
      actor_loss = -log_prob*(delta)
      critic_loss = -delta**2
      total_loss = actor_loss+critic_loss
      
      gradient = tape.gradient(total_loss,self.actor_critic.trainable_variables)
      self.actor_critic.optimizer.apply_gradients(zip(
          gradient,self.actor_critic.trainable_variables
      ))

from tqdm import tqdm

agent = Agent()
ep_rewards = []

if LOAD_CHECK:
  agent.load_model()
for episode in tqdm(range(1,TOTAL_EPISODES+1),ascii=True,unit='episodes'):
    visited_states_set = set()
    episode_reward = 0
    current_stop = DEPO
    current_state = convert_station_to_vector(DEPO)
#     print('here_1')
    
    done = False
    while not done:
        # if np.random.random()>EPSILON:
        action = agent.chose_an_action(current_state)
        # print(episode,action,visited_states_set)
            #print('here_2')
        # else:
        #     #print('here_3')
        #     action = np.random.randint(0,rewards.shape[0])
        #     while action==current_stop:
        #         action = np.random.randint(0,rewards.shape[0])
        #print('here_4')
        new_state = action
        new_state = convert_station_to_vector(action)
        visited_states_set.add(action)
        if len(visited_states_set) == 13:
            done=True
#         print(visited_states_set,done)
        reward = rewards[current_stop][action]
        episode_reward+=reward

        if not LOAD_CHECK:
          agent.learning(current_state,reward,new_state,done)


        current_state = new_state
        current_stop = action

    if not LOAD_CHECK:
      agent.save_model()
      
    
    ep_rewards.append(episode_reward)
    if not episode % AGGREGATE_STATS_EVERY or episode == 1:
        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])
        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])
        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])
    
    if EPSILON > MIN_EPSILON:
        EPSILON *= EPSILON_DECAY
        EPSILON = max(MIN_EPSILON, EPSILON)

# agent.load_model()

agent.make_prediction(convert_station_to_vector(11))

agent.make_prediction(convert_station_to_vector(0))[0]

np.argmax(agent.make_prediction(convert_station_to_vector(0))[1])

path=[0]
        all_states=[i for i in range(0,13)]
        while len(path)!=13:
          remaining_states=np.delete(all_states,path)
          action_index_in_remaining_states = np.argmax(np.delete(agent.make_prediction(convert_station_to_vector(path[-1]))[1], path))
          action = remaining_states[action_index_in_remaining_states]
          path.append(action)
        path

travel=0
for i in range(len(path)-1):
    travel+=distance[path[i]][path[i+1]]
travel

agent.actor_critic.trainable_variables

